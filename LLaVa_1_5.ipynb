{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hieubui1103/Construction_/blob/main/LLaVa_1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flash_attn\n"
      ],
      "metadata": {
        "id": "faj9CyxWiNtf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_L-WfgxMuiO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attention_2=True\n",
        ").to(0)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n",
        "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir temp_clone\n",
        "%cd temp_clone\n",
        "\n",
        "# Sparse-checkout from here\n",
        "!git clone --filter=blob:none --no-checkout https://github.com/Hieubui1103/Construction_.git\n",
        "%cd Construction_\n",
        "!git sparse-checkout init --cone\n",
        "!git sparse-checkout set Images\n",
        "!git checkout main\n",
        "\n",
        "# get back to the main directory\n",
        "%cd ../../"
      ],
      "metadata": {
        "id": "MO07POSDsP3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "# Path to the folder containing 1000 images\n",
        "image_folder_action = \"temp_clone/Construction_/Images/Action_cons_files\"\n",
        "output_folder_action = \"images_action\"\n",
        "\n",
        "image_folder_emotion = \"temp_clone/Construction_/Images/Emotion_cons_files\"\n",
        "output_folder_emotion = \"images_emotion\"\n",
        "\n",
        "# Create output folders if they don't exist\n",
        "os.makedirs(output_folder_action, exist_ok=True)\n",
        "os.makedirs(output_folder_emotion, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "6OAQVRm2-Hwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(image_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Create 5 batches of images from the specified folder and copy them into separate folders.\n",
        "    \"\"\"\n",
        "    # Create 5 batch folders\n",
        "    batch_size = 200\n",
        "    for batch_num in range(1, 6):\n",
        "        batch_folder = os.path.join(output_folder, f\"batch_{batch_num}\")\n",
        "        os.makedirs(batch_folder, exist_ok=True)\n",
        "\n",
        "    # Get all image files and sort them numerically\n",
        "    image_files = sorted([f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "    def extract_number(filename):\n",
        "        match = re.search(r'\\d+', filename)\n",
        "        return int(match.group()) if match else -1\n",
        "\n",
        "    image_files.sort(key=extract_number)\n",
        "\n",
        "    # Distribute images into batches\n",
        "    for i, file in enumerate(image_files):\n",
        "        batch_num = (i // batch_size) + 1\n",
        "        if batch_num > 5:  # Ensure no more than 5 batches\n",
        "            break\n",
        "        source_path = os.path.join(image_folder, file)\n",
        "        destination_path = os.path.join(output_folder, f\"batch_{batch_num}\", file)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "    print(\"Images have been copied into 5 batches.\")\n",
        "\n",
        "\n",
        "# Create batches for action images\n",
        "create_batches(image_folder_action, output_folder_action)\n",
        "\n",
        "# Create batches for emotion images\n",
        "create_batches(image_folder_emotion, output_folder_emotion)"
      ],
      "metadata": {
        "id": "62g_tz2f_rTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory\n",
        "image_dir_batch1 = \"images_action/batch_1\"\n",
        "image_dir_batch2 = \"images_action/batch_2\"\n",
        "image_dir_batch3 = \"images_action/batch_3\"\n",
        "image_dir_batch4 = \"images_action/batch_4\"\n",
        "image_dir_batch5 = \"images_action/batch_5\"\n",
        "\n",
        "emotion_dir_batch1 = \"images_emotion/batch_1\"\n",
        "emotion_dir_batch2 = \"images_emotion/batch_2\"\n",
        "emotion_dir_batch3 = \"images_emotion/batch_3\"\n",
        "emotion_dir_batch4 = \"images_emotion/batch_4\"\n",
        "emotion_dir_batch5 = \"images_emotion/batch_5\"\n",
        "# Get all files in the directory and sort them\n",
        "def get_sorted_files(image_dir):\n",
        "  image_files = sorted(\n",
        "      [os.path.join(image_dir, file) for file in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, file))],\n",
        "      key=lambda x: int(''.join(filter(str.isdigit, os.path.basename(x)))) if any(c.isdigit() for c in os.path.basename(x)) else x\n",
        "  )\n",
        "  return image_files\n",
        "\n",
        "print(get_sorted_files(image_dir_batch5))  # Prints a sorted list of image file paths\n"
      ],
      "metadata": {
        "id": "2m7C29u4MUql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "# Define the JSON file name\n",
        "\n",
        "for i in range(1, 6):\n",
        "  json_filename_action = f\"response_action_LLaVa_{i}.json\"\n",
        "  json_filename_emotion = f\"response_emotion_LLaVa_{i}.json\"\n",
        "  # Create an empty JSON structure\n",
        "  data = {}\n",
        "\n",
        "  # Write the empty structure to the file\n",
        "  with open(json_filename_action, \"w\") as file:\n",
        "      json.dump(data, file, indent=4)\n",
        "  with open(json_filename_emotion, \"w\") as file:\n",
        "      json.dump(data, file, indent=4)\n",
        "\n",
        "  print(f\"{json_filename_action} created successfully!\")\n",
        "  print(f\"{json_filename_emotion} created successfully!\")\n"
      ],
      "metadata": {
        "id": "43finkZnRM4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(data_dict, data,image, json_filename):\n",
        "    print(image)\n",
        "    data_dict[image] = data\n",
        "    with open(json_filename, \"w\") as file:\n",
        "        json.dump(data_dict, file, indent=4)\n",
        "\n",
        "    print(f\"Response saved in {json_filename}\")\n"
      ],
      "metadata": {
        "id": "euGaSen2Rt1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# action classes\n",
        "jobs = [\n",
        "        \"Operating heavy machinery\",\n",
        "                    \"Lifting materials\",\n",
        "                    \"Measuring and marking surfaces\",\n",
        "                    \"Mixing cement\",\n",
        "                    \"Collaborating in teams\",\n",
        "                    \"Using hand tools\",\n",
        "                    \"Inspecting completed work\",\n",
        "                    \"Communicating with supervisors\",\n",
        "                    \"Following safety protocols\",\n",
        "                    \"Taking breaks/resting\"\n",
        "        ]\n",
        "\n",
        "# emotion classes\n",
        "emotions = [\n",
        "        \"Focused\",\n",
        "        \"Determined\",\n",
        "        \"Tired\",\n",
        "        \"Alert\",\n",
        "        \"Satisfied\",\n",
        "        \"Anxious\",\n",
        "        \"Proud\",\n",
        "        \"Frustrated\",\n",
        "        \"Cooperative\",\n",
        "        \"Relieved\"\n",
        "        ]"
      ],
      "metadata": {
        "id": "4eYK2DHlXHmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer retrieval with direct questioning"
      ],
      "metadata": {
        "id": "A520SPMjBh_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_ans(image_files, json_file, category, classes):\n",
        "  data = {}\n",
        "  for image in image_files:\n",
        "    print(f\"\\n Current process image: {image}  \\n\")\n",
        "    answer_data_for_each = []\n",
        "    for job in classes:\n",
        "      conversation = [\n",
        "          {\n",
        "\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": f\"\"\" Does this {category} of '{job}' appear in this image? Only answer yes or no\"\"\"},\n",
        "                {\"type\": \"image\"},\n",
        "              ],\n",
        "          },\n",
        "      ]\n",
        "      prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "      raw_image = Image.open(image)\n",
        "      inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
        "\n",
        "      output = model.generate(**inputs, max_new_tokens=10000, do_sample=False)\n",
        "      response_1 = processor.decode(output[0][2:], skip_special_tokens=True)\n",
        "\n",
        "      match = re.search(r\"ASSISTANT:\\s*(\\w+)\", response_1)\n",
        "      if match:\n",
        "          answer = match.group(1)\n",
        "          answer_data_for_each.append(answer)\n",
        "      else:\n",
        "          print(\"No match found.\")\n",
        "      print(response_1)\n",
        "\n",
        "      # Clear cache\n",
        "      del inputs, output, response_1, match\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    print(answer_data_for_each)\n",
        "    save_data(data, answer_data_for_each,image, json_file)\n",
        "    del answer_data_for_each\n",
        "    torch.cuda.empty_cache()\n",
        "  del data\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wMKJCCJSiMtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get data for action recognition\n"
      ],
      "metadata": {
        "id": "Rb77f9u-Baly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(image_dir_batch1), \"response_action_LLaVa_1.json\", \"action\", jobs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5JPWY43q6h6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(image_dir_batch2), \"response_action_LLaVa_2.json\", \"action\", jobs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DvnHomI8Z0vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(image_dir_batch3), \"response_action_LLaVa_3.json\", \"action\", jobs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eEmludjnZ3L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(image_dir_batch4), \"response_action_LLaVa_4.json\", \"action\", jobs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_gniRC4uhyFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(image_dir_batch5), \"response_action_LLaVa_5.json\", \"action\", jobs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tMHveDBUkN2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get data for emotion recognition"
      ],
      "metadata": {
        "id": "OEI3-wTxBekC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(emotion_dir_batch1), \"response_emotion_LLaVa_1.json\", \"emotion\", emotions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SO_spCujn3My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(emotion_dir_batch2), \"response_emotion_LLaVa_2.json\", \"emotion\", emotions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WXaP_j0RxAQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(emotion_dir_batch3), \"response_emotion_LLaVa_3.json\", \"emotion\", emotions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UxWYDACExCFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(emotion_dir_batch4), \"response_emotion_LLaVa_4.json\", \"emotion\", emotions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Y5rbrGhrxDbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_ans(get_sorted_files(emotion_dir_batch5), \"response_emotion_LLaVa_5.json\", \"emotion\", emotions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7G9smrDQxExh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOom7gRhJ/OjOz0Pc0q8sAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}